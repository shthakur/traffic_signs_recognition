{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim, nn\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 43\n",
    "IMG_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = Namespace()\n",
    "params.data = './data'\n",
    "params.lr = 0.0001\n",
    "params.batch_size = 64\n",
    "params.seed = 7\n",
    "params.cnn = '100, 150, 250, 350'\n",
    "params.locnet = '200,300,200'\n",
    "params.locnet2 = None\n",
    "params.locnet3 = '150,150,150'\n",
    "params.st = True\n",
    "params.resume = False\n",
    "params.epochs = 15\n",
    "params.patience = 10\n",
    "params.dropout = 0.5\n",
    "params.use_pickle = True\n",
    "params.save_loc = \"/scratch/as10656/\"\n",
    "params.outfile = 'gtsrb_kaggle.csv'\n",
    "params.train_pickle = params.save_loc + '/train_balanced_preprocessed.p'\n",
    "params.extra_debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed(params.seed)\n",
    "torch.manual_seed(params.seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrafficSignsDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = torch.from_numpy(images)\n",
    "        self.images = self.images.permute(0, 3, 1, 2)\n",
    "        self.labels = torch.LongTensor(labels.argmax(1))\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.images[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from skimage import exposure\n",
    "from PIL import Image\n",
    "\n",
    "import warnings \n",
    "\n",
    "class Utils:\n",
    "    def __init__(self):\n",
    "        self.train_data_transforms = transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        self.val_data_transforms = transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def load_pickled_data(self, file, columns):\n",
    "        with open(file, mode='rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "        return tuple(map(lambda c: dataset[c], columns))\n",
    "    \n",
    "    def get_dataset(self, params):\n",
    "        if params.use_pickle:\n",
    "            data_images, data_labels = self.load_pickled_data(params.train_pickle, ['features', 'labels'])\n",
    "            train_images, val_images, train_labels, val_labels = train_test_split(data_images, \n",
    "                                                                                  data_labels, \n",
    "                                                                                  test_size=0.25) \n",
    "            return TrafficSignsDataset(train_images, train_labels), TrafficSignsDataset(val_images, val_labels)\n",
    "        else:\n",
    "            train_dataset = datasets.ImageFolder(params.data + '/train_images',\n",
    "                                                 transform=self.train_data_transforms)\n",
    "            val_dataset = datasets.ImageFolder(self.params.data + '/val_images',\n",
    "                                               transform=self.val_data_transforms)\n",
    "            return train_dataset, val_dataset\n",
    "    \n",
    "    def pickle_data(self, x, y, save_loc):\n",
    "        print(\"Saving pickle at \" + save_loc)\n",
    "        save = {\"features\": x, \"labels\": y}\n",
    "        \n",
    "        with open(save_loc, \"wb\") as f:\n",
    "            pickle.dump(save, f)\n",
    "    \n",
    "    def pickle_data_from_folder(self, data_folder, save_loc):\n",
    "        if not os.path.isdir(data_folder):\n",
    "            print(\"Data folder must be a folder and should contains sub folders for each label\")\n",
    "            return\n",
    "        \n",
    "        resize_transform = transforms.Resize((IMG_SIZE, IMG_SIZE))\n",
    "        sub_folders = os.listdir(data_folder)\n",
    "        \n",
    "        count = 0\n",
    "        for sub_folder in sub_folders:\n",
    "            sub_folder = os.path.join(data_folder, sub_folder)\n",
    "\n",
    "            if not os.path.isdir(sub_folder):\n",
    "                continue\n",
    "            label = int(sub_folder.split(\"/\")[-1])\n",
    "\n",
    "            for image in os.listdir(sub_folder):\n",
    "                count += 1\n",
    "\n",
    "        save = {\"features\": np.empty([count, IMG_SIZE, IMG_SIZE, 3], dtype=np.uint8), \n",
    "                \"labels\": np.empty([count], dtype=int)}\n",
    "        i = 0\n",
    "        for sub_folder in sub_folders:\n",
    "            sub_folder = os.path.join(data_folder, sub_folder)\n",
    "\n",
    "            if not os.path.isdir(sub_folder):\n",
    "                continue\n",
    "            label = int(sub_folder.split(\"/\")[-1])\n",
    "            for image in os.listdir(sub_folder):\n",
    "                image = os.path.join(sub_folder, image)\n",
    "                pic = Image.open(image)\n",
    "                pic = resize_transform(pic)\n",
    "                pic = np.array(pic)\n",
    "                save[\"features\"][i] = pic\n",
    "                save[\"labels\"][i] = label\n",
    "                i += 1\n",
    "\n",
    "        \n",
    "        with open(save_loc, \"wb\") as f:\n",
    "            pickle.dump(save, f)\n",
    "    \n",
    "    def get_dataset_from_file(self, file):\n",
    "        data_images, data_labels = self.load_pickled_data(file, ['features', 'labels'])\n",
    "        \n",
    "        return TrafficSignsDataset(data_images, data_labels)\n",
    "\n",
    "    def get_convnet_output_size(self, network, input_size=IMG_SIZE):\n",
    "        input_size = input_size or IMG_SIZE\n",
    "\n",
    "        if type(network) != list:\n",
    "            network = [network]\n",
    "\n",
    "        in_channels = network[0].conv.in_channels\n",
    "\n",
    "        output = Variable(torch.ones(1, in_channels, input_size, input_size))\n",
    "        output.require_grad = False\n",
    "        for conv in network:\n",
    "            output = conv.forward(output)\n",
    "\n",
    "        return np.asscalar(np.prod(output.data.shape)), output.data.size()[2]\n",
    "    def get_time_hhmmss(self, start = None):\n",
    "        \"\"\"\n",
    "        Calculates time since `start` and formats as a string.\n",
    "        \"\"\"\n",
    "        if start is None:\n",
    "            return time.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "        end = time.time()\n",
    "        m, s = divmod(end - start, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        time_str = \"%02d:%02d:%02d\" % (h, m, s)\n",
    "        return time_str \n",
    "    \n",
    "        \n",
    "    def save_checkpoint(self, state, is_best, filename='checkpoint.pth.tar'):\n",
    "        torch.save(state, filename)\n",
    "        epoch = state['epoch']\n",
    "        print(\"=> Saving model to %s\" % filename)\n",
    "\n",
    "        if is_best:\n",
    "            print(\"=> The model just saved has performed best on validation set\" +\n",
    "                  \" till now\")\n",
    "            shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "        \n",
    "        return filename\n",
    "\n",
    "\n",
    "    def load_checkpoint(self, resume):\n",
    "        if os.path.isfile(resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "            \n",
    "            if not torch.cuda.is_available():\n",
    "                checkpoint = torch.load(resume, map_location=lambda storage, location: storage)\n",
    "            else:\n",
    "                checkpoint = torch.load(resume)\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                     .format(resume, checkpoint['epoch']))\n",
    "            return checkpoint\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def preprocess_dataset(self, X, y=None, use_tqdm=True):\n",
    "        # Convert to single channel Y\n",
    "        X = 0.299 * X[:, :, :, 0] + 0.587 * X[:, :, :, 1] + 0.114 * X[:, :, :, 2]\n",
    "        \n",
    "        # Scale\n",
    "        X = (X / 255.).astype(np.float32)\n",
    "        \n",
    "        # Don't want to use tqdm while generating csv\n",
    "        if use_tqdm:\n",
    "            preprocess_range = tqdm(range(X.shape[0]))\n",
    "        else:\n",
    "            preprocess_range = range(X.shape[0])\n",
    "            \n",
    "        # Ignore warnings, see http://scikit-image.org/docs/dev/user_guide/data_types.html\n",
    "        for i in preprocess_range:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                X[i] = exposure.equalize_adapthist(X[i])\n",
    "\n",
    "        if y is not None:  \n",
    "            # Convert to one-hot encoding. Convert back with `y = y.nonzero()[1]`\n",
    "            y = np.eye(NUM_CLASSES)[y]\n",
    "            X, y = shuffle(X, y)\n",
    "\n",
    "        # Add a single grayscale channel\n",
    "        X = X.reshape(X.shape + (1,)) \n",
    "        return X, y\n",
    "\n",
    "utils = Utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.transform import rotate, warp, ProjectiveTransform\n",
    "import random\n",
    "\n",
    "class Extender:\n",
    "    def __init__(self, data_images, data_labels, ratio=0.5, intensity=0.75):\n",
    "        self.x = data_images\n",
    "        self.y = data_labels\n",
    "        self.intensity = intensity\n",
    "        self.ratio = ratio\n",
    "        \n",
    "        # These classes can be horizontally flipped for new images\n",
    "        # Contains yield sign, ice, traffic signals etc\n",
    "        self.horizontally_flippable_classes = [11, 12, 13, 15, 17, 18, \n",
    "                                               22, 26, 30, 35]\n",
    "        \n",
    "        # These ones can vertically flipped for new images\n",
    "        # Contains 30 limit, 80 limit, no vehicles\n",
    "        self.vertically_flippable_classes = np.array([1, 5, 12, 15, 17])\n",
    "        \n",
    "        # These ones need to be flipped both vertically and horizontally\n",
    "        # to get same image\n",
    "        self.both_flippable = [32, 40]\n",
    "        \n",
    "        # These ones contains pair which first can be generated by \n",
    "        # horizontally flipping the other. For e.g turn left and \n",
    "        # turn right\n",
    "        self.flip_exchangeable = np.array([\n",
    "            (19, 20),\n",
    "            (20, 19),\n",
    "            (33, 34),\n",
    "            (34, 33),\n",
    "            (36, 37),\n",
    "            (37, 36),\n",
    "            (38, 39),\n",
    "            (39, 38)\n",
    "        ])\n",
    "    \n",
    "    def extend_and_balance(self, custom_counts=None):\n",
    "        print(\"Extending and balancing dataset with intesity\", self.intensity)\n",
    "        x, y = self.flip()\n",
    "        _, class_counts = np.unique(y, return_counts=True)\n",
    "        max_count = max(class_counts)\n",
    "        \n",
    "        if custom_counts is None:\n",
    "            total = max_count * NUM_CLASSES\n",
    "        else:\n",
    "            total = np.sum(custom_counts)\n",
    "        \n",
    "        x_balanced = np.empty([0, x.shape[1], x.shape[2], x.shape[3]],\n",
    "                              dtype=np.float32)\n",
    "        y_balanced = np.empty([0], dtype=y.dtype)\n",
    "        \n",
    "        for c, class_count in zip(range(NUM_CLASSES), tqdm(class_counts)):\n",
    "            x_org = (x[y == c] / 255.).astype(np.float32)\n",
    "            y_org = y[y == c]\n",
    "            \n",
    "            x_balanced = np.append(x_balanced, x_org, axis=0)\n",
    "            \n",
    "            max_count = max_count if custom_counts is None else custom_counts[c]\n",
    "            for i in range(max_count // class_count):\n",
    "                x_mod = self.rotate(x_org)\n",
    "                x_mod = self.projection_transform(x_mod)\n",
    "                x_balanced = np.append(x_balanced, x_mod, axis=0)\n",
    "            \n",
    "            if max_count % class_count > 0:\n",
    "                x_mod = self.rotate(x_org[:(max_count % class_count)])\n",
    "                x_mod = self.projection_transform(x_mod)\n",
    "            \n",
    "                x_balanced = np.append(x_balanced, x_mod, axis=0)\n",
    "            \n",
    "            extension = np.full(x_balanced.shape[0] - y_balanced.shape[0],\n",
    "                                c, dtype=y_balanced.dtype)\n",
    "            y_balanced = np.append(y_balanced, extension)\n",
    "            \n",
    "            del x_org\n",
    "            del y_org\n",
    "        \n",
    "        return (x_balanced * 255).astype(np.uint8), y_balanced\n",
    "    \n",
    "    def flip(self):\n",
    "        x = np.empty([0, self.x.shape[1], self.x.shape[2], \n",
    "                      self.x.shape[3]], \n",
    "                      dtype=self.x.dtype)\n",
    "        y = np.empty([0], dtype=self.y.dtype)\n",
    "        \n",
    "        for c in range(NUM_CLASSES):\n",
    "            # Add existing data\n",
    "            x = np.append(x, self.x[self.y == c], axis=0)\n",
    "            \n",
    "            if c in self.horizontally_flippable_classes:\n",
    "                # Flip columns and append\n",
    "                x = np.append(x, self.x[self.y == c][:, :, ::-1, :], \n",
    "                              axis=0)\n",
    "                \n",
    "            if c in self.vertically_flippable_classes:\n",
    "                # Flip rows and append\n",
    "                x = np.append(x, self.x[self.y == c][:, ::-1, :, :],\n",
    "                              axis=0)\n",
    "            \n",
    "            if c in self.flip_exchangeable[:, 0]:\n",
    "                flip_c = self.flip_exchangeable[self.flip_exchangeable[:, 0] == c]\n",
    "                flip_c = flip_c[0][1]\n",
    "                \n",
    "                # Flip other class horizontally \n",
    "                x = np.append(x, self.x[self.y == flip_c][:, :, ::-1, :], \n",
    "                              axis=0)\n",
    "            \n",
    "            if c in self.both_flippable:\n",
    "                # Flip both rows and columns\n",
    "                x = np.append(x, self.x[self.y == c][:, ::-1, ::-1, :],\n",
    "                             axis=0)\n",
    "            \n",
    "            # Extend y now\n",
    "            y = np.append(y, np.full(x.shape[0] - y.shape[0], c, \n",
    "                                     dtype=int))\n",
    "        \n",
    "        return (x, y)\n",
    "    \n",
    "    def rotate(self, x):\n",
    "        indices = np.random.choice(x.shape[0], int(x.shape[0] * self.ratio),\n",
    "                                   replace=False)\n",
    "        \n",
    "        # If we rotate more than 30 degrees, context is lost.\n",
    "        change = 30. * self.intensity\n",
    "        x_return = np.empty(x.shape, dtype=x.dtype)\n",
    "        for i in indices:\n",
    "            x_return[i] = rotate(x[i], random.uniform(-change, change), mode=\"edge\")\n",
    "        \n",
    "        return x_return\n",
    "    \n",
    "    def projection_transform(self, x):\n",
    "        image_size = x.shape[1]\n",
    "        \n",
    "        change = image_size * 0.3 * self.intensity\n",
    "        \n",
    "        x_return = np.empty(x.shape, dtype=x.dtype)\n",
    "        \n",
    "        indices = np.random.choice(x.shape[0], int(x.shape[0] * self.ratio),\n",
    "                                   replace=False)\n",
    "        for i in indices:\n",
    "            changes = []\n",
    "            for _ in range(8):\n",
    "                changes.append(random.uniform(-change, change))\n",
    "            \n",
    "            transform = ProjectiveTransform()\n",
    "            transform.estimate(np.array(\n",
    "                (\n",
    "                    (changes[0], changes[1]), # top left\n",
    "                    (changes[2], image_size - changes[3]), # bottom left\n",
    "                    (image_size - changes[4], changes[5]), # top right\n",
    "                    (image_size - changes[6], image_size - changes[7]) # bottom right\n",
    "                )), np.array(\n",
    "                (\n",
    "                    (0, 0),\n",
    "                    (0, image_size),\n",
    "                    (image_size, 0),\n",
    "                    (image_size, image_size)\n",
    "                ))\n",
    "            )\n",
    "            \n",
    "            x_return[i] = warp(x[i], transform, output_shape=(image_size, image_size),\n",
    "                        order=1, mode=\"edge\")\n",
    "        \n",
    "        return x_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the block below and run for extended, balanced and preprocessed images generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|‚ñè         | 9607/695980 [01:30<1:47:16, 106.64it/s]"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_pickle_path = params.save_loc + \"/train.p\"\n",
    "# utils.pickle_data_from_folder(params.data + \"train_images\", train_pickle_path)\n",
    "# x, y = utils.load_pickled_data(train_pickle_path, [\"features\", \"labels\"])\n",
    "# print(x.shape)\n",
    "extender = Extender(x, y, 1, 0.75)\n",
    "x_extended, y_extended = extender.flip()\n",
    "utils.pickle_data(x_extended, y_extended, params.save_loc + \"train_extended.p\")\n",
    "\n",
    "# Generate 10k augmented plus original images for each class\n",
    "x_balanced, y_balanced = extender.extend_and_balance(custom_counts=np.array([10000] * NUM_CLASSES))\n",
    "utils.pickle_data(x_balanced, y_balanced, params.save_loc + \"train_balanced.p\")\n",
    "\n",
    "x_preprocessed, y_preprocessed = utils.preprocess_dataset(x_balanced, y_balanced)\n",
    "utils.pickle_data(x_preprocessed, y_preprocessed, params.save_loc + \"train_balanced_preprocessed.p\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run the block below if we are preprocessing in the above block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# View and run only after above\n",
    "\"\"\"\n",
    "plt.imshow(x_preprocessed[-21000].reshape(32, 32), cmap=\"gray\")\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# Would be different as preprocessed was shuffled\n",
    "\"\"\"\n",
    "plt.imshow(x_balanced[-21000])\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_c, out_c,\n",
    "                 kernel_size,\n",
    "                 padding_size='same',\n",
    "                 pool_stride=2,\n",
    "                 batch_norm=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if padding_size == 'same':\n",
    "            padding_size = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(in_c, out_c, kernel_size, padding=padding_size)\n",
    "        self.max_pool2d = nn.MaxPool2d(pool_stride, stride=pool_stride)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.batch_norm_2d = nn.BatchNorm2d(out_c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.max_pool2d(nn.functional.leaky_relu(self.conv(x)))\n",
    "\n",
    "        if self.batch_norm:\n",
    "            return self.batch_norm_2d(x)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_nbr, out_nbr):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.input_nbr = input_nbr\n",
    "        self.lin = nn.Linear(input_nbr, out_nbr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftMaxClassifier(Classifier):\n",
    "    def __init__(self, in_len, out_len):\n",
    "        super().__init__(in_len, out_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x)\n",
    "        return nn.functional.log_softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, input_nbr, out_nbr):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        self.input_nbr = input_nbr\n",
    "        self.lin = nn.Linear(input_nbr, out_nbr)\n",
    "        self.rel = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.dropout(self.rel(self.lin(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LocalizationNetwork(nn.Module):\n",
    "    nbr_params = 6\n",
    "    init_bias = torch.Tensor([1, 0, 0, 0, 1, 0])\n",
    "\n",
    "    def __init__(self, conv_params, kernel_sizes,\n",
    "                 input_size, input_channels=1):\n",
    "        super(LocalizationNetwork, self).__init__()\n",
    "\n",
    "        if not kernel_sizes:\n",
    "            kernel_sizes = [5, 5]\n",
    "\n",
    "        if len(kernel_sizes) != 2:\n",
    "            raise Exception(\"Number of kernel sizes != 2\")\n",
    "\n",
    "        self.conv1 = ConvNet(input_channels, conv_params[0],\n",
    "                             kernel_size=kernel_sizes[0],\n",
    "                             batch_norm=False)\n",
    "        self.conv2 = ConvNet(conv_params[0], conv_params[1],\n",
    "                             kernel_size=kernel_sizes[1],\n",
    "                             batch_norm=False)\n",
    "        conv_output_size, _ = utils.get_convnet_output_size([self.conv1, self.conv2],\n",
    "                                                            input_size)\n",
    "\n",
    "        self.fc = FullyConnected(conv_output_size, conv_params[2])\n",
    "        self.classifier = Classifier(conv_params[2], self.nbr_params)\n",
    "\n",
    "        self.classifier.lin.weight.data.fill_(0)\n",
    "        self.classifier.lin.bias.data = torch.FloatTensor([1, 0, 0, 0, 1, 0])\n",
    "        self.dropout = nn.Dropout2d()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.conv1(x))\n",
    "        conv_output = self.dropout(self.conv2(x))\n",
    "        conv_output = conv_output.view(conv_output.size()[0], -1)\n",
    "        return self.classifier(self.fc(conv_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SpatialTransformerNetwork(nn.Module):\n",
    "    def __init__(self, params, kernel_sizes, input_size=IMG_SIZE,\n",
    "                 input_channels=1):\n",
    "        super(SpatialTransformerNetwork, self).__init__()\n",
    "        self.localization_network = LocalizationNetwork(params,\n",
    "                                                        kernel_sizes,\n",
    "                                                        input_size,\n",
    "                                                        input_channels)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.localization_network(input)\n",
    "        out = out.view(out.size()[0], 2, 3)\n",
    "        grid = nn.functional.affine_grid(out, input.size())\n",
    "        return nn.functional.grid_sample(input, grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GeneralNetwork(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(GeneralNetwork, self).__init__()\n",
    "\n",
    "        if not opt.cnn:\n",
    "            opt.cnn = '100, 150, 250, 350'\n",
    "        self.kernel_sizes = [5, 3, 1]\n",
    "        conv_params = list(map(int, opt.cnn.split(\",\")))\n",
    "\n",
    "        self.conv1 = ConvNet(1, conv_params[0], kernel_size=self.kernel_sizes[0],\n",
    "                             padding_size=0)\n",
    "        self.conv2 = ConvNet(conv_params[0], conv_params[1],\n",
    "                             kernel_size=self.kernel_sizes[1],\n",
    "                             padding_size=0)\n",
    "\n",
    "        conv_output_size, _ = utils.get_convnet_output_size([self.conv1, self.conv2])\n",
    "\n",
    "        self.fc = FullyConnected(conv_output_size, conv_params[2])\n",
    "        self.classifier = SoftMaxClassifier(conv_params[2], NUM_CLASSES)\n",
    "\n",
    "        self.locnet_1 = None\n",
    "        if opt.st and opt.locnet:\n",
    "            params = list(map(int, opt.locnet.split(\",\")))\n",
    "            self.locnet_1 = SpatialTransformerNetwork(params,\n",
    "                                                      kernel_sizes=[7, 5])\n",
    "\n",
    "        self.locnet_2 = None\n",
    "        if opt.st and opt.locnet2:\n",
    "            params = list(map(int, opt.locnet2.split(\",\")))\n",
    "            _, current_size = utils.get_convnet_output_size([self.conv1])\n",
    "            self.locnet_2 = SpatialTransformerNetwork(params,\n",
    "                                                      [5, 3],\n",
    "                                                      current_size,\n",
    "                                                      conv_params[0])\n",
    "        self.dropout = nn.Dropout2d()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.locnet_1:\n",
    "            x = self.locnet_1(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        if self.locnet_2:\n",
    "            x = self.locnet_2(x)\n",
    "\n",
    "        return self.classifier(self.fc(self.dropout(self.conv2(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IDSIANetwork(GeneralNetwork):\n",
    "    def __init__(self, opt):\n",
    "        super().__init__(opt)\n",
    "        conv_params = list(map(int, opt.cnn.split(\",\")))\n",
    "\n",
    "        self.conv3 = ConvNet(conv_params[1], conv_params[2], kernel_size=self.kernel_sizes[2],\n",
    "                             padding_size=0)\n",
    "        conv_output_size, _ = utils.get_convnet_output_size([self.conv1,\n",
    "                                                      self.conv2,\n",
    "                                                      self.conv3])\n",
    "        self.fc = FullyConnected(conv_output_size, conv_params[3])\n",
    "        self.classifier = SoftMaxClassifier(conv_params[3], NUM_CLASSES)\n",
    "\n",
    "        self.locnet_3 = None\n",
    "        if opt.st and opt.locnet3:\n",
    "            params = list(map(int, opt.locnet3.split(\",\")))\n",
    "            _, current_size = utils.get_convnet_output_size([self.conv1, self.conv2])\n",
    "            self.locnet_3 = SpatialTransformerNetwork(params,\n",
    "                                                      [3, 3],\n",
    "                                                      current_size,\n",
    "                                                      conv_params[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.locnet_1:\n",
    "            x = self.locnet_1(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        if self.locnet_2:\n",
    "            x = self.locnet_2(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        if self.locnet_3:\n",
    "            x = self.locnet_3(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return self.classifier(self.fc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, model, optimizer, params=None, patience=100, minimize=True):\n",
    "        self.minimize = minimize\n",
    "        self.patience = patience\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.params = params\n",
    "        self.best_monitored_value = np.inf if minimize else 0.\n",
    "        self.best_monitored_acc = 0. if minimize else np.inf\n",
    "        self.best_monitored_epoch = 0\n",
    "\n",
    "        self.restore_path = None\n",
    "\n",
    "    def __call__(self, value, acc, epoch, rest):\n",
    "        if (self.minimize and value < self.best_monitored_value) or (not self.minimize and value > self.best_monitored_value):\n",
    "            self.best_monitored_value = value\n",
    "            self.best_monitored_epoch = epoch\n",
    "            self.best_monitored_acc = acc\n",
    "\n",
    "            state = {\n",
    "                'params': self.params,\n",
    "                'epoch': self.best_monitored_epoch,\n",
    "                'state_dict': self.model.state_dict(),\n",
    "                'best': self.best_monitored_value,\n",
    "                'best_acc': self.best_monitored_acc,\n",
    "                'optimizer': self.optimizer.state_dict(),\n",
    "            }\n",
    "\n",
    "            rest.update(state)\n",
    "            print(value, acc)\n",
    "            self.restore_path = utils.save_checkpoint(\n",
    "                rest, True, os.path.join(self.params.save_loc, \"early_stopping_checkpoint\"))\n",
    "        elif self.best_monitored_epoch + self.patience < epoch:\n",
    "            if self.restore_path is not None:\n",
    "                checkpoint = utils.load_checkpoint(self.restore_path)\n",
    "                self.best_monitored_value = checkpoint['best']\n",
    "                self.best_monitored_acc = checkpoint['best_acc']\n",
    "                self.best_monitored_epoch = checkpoint['epoch']\n",
    "                self.model.load_state_dict(checkpoint['state_dict'])\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            else:\n",
    "                print(\"ERROR: Failed to restore session\")\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def init_from_checkpoint(self, checkpoint):\n",
    "        self.best_monitored_value = checkpoint['best']\n",
    "        self.best_monitored_acc = checkpoint['best_acc']\n",
    "        self.best_monitored_epoch = 0\n",
    "\n",
    "        if \"params\" in checkpoint:\n",
    "            self.params = checkpoint['params']\n",
    "\n",
    "    def print_info(self):\n",
    "        print(\"Best loss: {0}, Best Accuracy: {1}, at epoch {2}\"\n",
    "              .format(self.best_monitored_value,\n",
    "                      self.best_monitored_acc,\n",
    "                      self.best_monitored_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    def plot_curve(self, sub_plot, params, train_column, valid_column, linewidth = 2, train_linestyle = \"b-\", valid_linestyle = \"g-\"):\n",
    "        train_values = params[train_column]\n",
    "        valid_values = params[valid_column]\n",
    "        epochs = train_values.shape[0]\n",
    "        x_axis = np.arange(epochs)\n",
    "        sub_plot.plot(x_axis[train_values > 0], train_values[train_values > 0], train_linestyle, linewidth=linewidth, label=\"train\")\n",
    "        sub_plot.plot(x_axis[valid_values > 0], valid_values[valid_values > 0], valid_linestyle, linewidth=linewidth, label=\"valid\")\n",
    "        return epochs\n",
    "\n",
    "    # Plot history curves\n",
    "    def plot_learning_curves(self, params):\n",
    "        curves_figure = plt.figure(figsize = (10, 4))\n",
    "        sub_plot = curves_figure.add_subplot(1, 2, 1)\n",
    "        epochs_plotted = self.plot_curve(sub_plot, params, train_column = \"train_acc\", valid_column = \"val_acc\")\n",
    "\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.xlim(0, epochs_plotted)\n",
    "\n",
    "        sub_plot = curves_figure.add_subplot(1, 2, 2)\n",
    "        epochs_plotted = self.plot_curve(sub_plot, params, train_column = \"train_loss\", valid_column = \"val_loss\")\n",
    "\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlim(0, epochs_plotted)\n",
    "        plt.yscale(\"log\")\n",
    "plotter = Plotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, params, train_data=None, val_data=None):\n",
    "        self.params = params\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "\n",
    "        print(\"Creating dataloaders\")\n",
    "        self.cuda_available = torch.cuda.is_available()\n",
    "\n",
    "        if self.train_data is not None:\n",
    "            self.train_loader = DataLoader(dataset=self.train_data,\n",
    "                                           shuffle=True,\n",
    "                                           batch_size=params.batch_size,\n",
    "                                           pin_memory=self.cuda_available)\n",
    "        if self.val_data is not None:\n",
    "            self.val_loader = DataLoader(dataset=self.val_data,\n",
    "                                         shuffle=False,\n",
    "                                         batch_size=params.batch_size,\n",
    "                                         pin_memory=self.cuda_available)\n",
    "\n",
    "        self.string_fixer = \"==========\"\n",
    "\n",
    "    def load(self):\n",
    "        print(\"Loading model\")\n",
    "        self.model = IDSIANetwork(self.params)\n",
    "        self.optimizer = optim.Adam(filter(lambda p: p.requires_grad,\n",
    "                                           self.model.parameters()),\n",
    "                                    lr=self.params.lr)\n",
    "\n",
    "        self.start_time = time.time()\n",
    "        self.histories = {\n",
    "            \"train_loss\": np.empty(0, dtype=np.float32),\n",
    "            \"train_acc\": np.empty(0, dtype=np.float32),\n",
    "            \"val_loss\": np.empty(0, dtype=np.float32),\n",
    "            \"val_acc\": np.empty(0, dtype=np.float32)\n",
    "        }\n",
    "\n",
    "        # We minimize the cross entropy loss here\n",
    "        self.early_stopping = EarlyStopping(\n",
    "            self.model, self.optimizer, params=self.params,\n",
    "            patience=self.params.patience, minimize=True)\n",
    "\n",
    "        if self.params.resume:\n",
    "            checkpoint = utils.load_checkpoint(self.params.resume)\n",
    "            if checkpoint is not None:\n",
    "\n",
    "                if \"params\" in checkpoint:\n",
    "                    # To make sure model architecture remains same\n",
    "                    self.params.locnet = checkpoint['params'].locnet\n",
    "                    self.params.locnet2 = checkpoint['params'].locnet2\n",
    "                    self.params.locnet3 = checkpoint['params'].locnet3\n",
    "                    self.params.st = checkpoint['params'].st\n",
    "\n",
    "                    self.model = IDSIANetwork(self.params)\n",
    "                    self.optimizer = optim.Adam(filter(lambda p: p.requires_grad,\n",
    "                                                       self.model.parameters()),\n",
    "                                                lr=self.params.lr)\n",
    "\n",
    "                self.model.load_state_dict(checkpoint['state_dict'])\n",
    "                \n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "                self.histories.update(checkpoint)\n",
    "                self.early_stopping.init_from_checkpoint(checkpoint)\n",
    "                print(\"Loaded model, Best Loss: %.8f, Best Acc: %.2f\" %\n",
    "                      (checkpoint['best'], checkpoint['best_acc']))\n",
    "\n",
    "        if self.cuda_available:\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "    def train(self):\n",
    "        self.epochs = self.params.epochs\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        start_epoch = 0\n",
    "\n",
    "        self.model.train()\n",
    "        print(\"Starting training\")\n",
    "        self.print_info()\n",
    "        for epoch in range(start_epoch, self.params.epochs):\n",
    "            for i, (images, labels) in enumerate(self.train_loader):\n",
    "                images_batch = Variable(images)\n",
    "                labels_batch = Variable(labels)\n",
    "\n",
    "                if self.cuda_available:\n",
    "                    images_batch = images_batch.cuda()\n",
    "                    labels_batch = labels_batch.cuda(async=True)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(images_batch)\n",
    "                loss = criterion(output, labels_batch.long())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                if self.params.extra_debug and (i + 1) % (self.params.batch_size * 4) == 0:\n",
    "                    print(('Epoch: [{0}/{1}], Step: [{2}/{3}], Loss: {4},')\n",
    "                          .format(epoch + 1,\n",
    "                                  self.params.epochs,\n",
    "                                  i + 1,\n",
    "                                  len(self.train_loader),\n",
    "                                  loss.data[0]))\n",
    "\n",
    "            train_acc, train_loss = self.validate_model(self.train_loader, self.model)\n",
    "            val_acc, val_loss = self.validate_model(self.val_loader, self.model)\n",
    "\n",
    "            self.histories['train_loss'] = np.append(self.histories['train_loss'], [train_loss])\n",
    "            self.histories['val_loss'] = np.append(self.histories['val_loss'], [val_loss])\n",
    "            self.histories['val_acc'] = np.append(self.histories['val_acc'], [val_acc])\n",
    "            self.histories['train_acc'] = np.append(self.histories['train_acc'], [train_acc])\n",
    "\n",
    "            if not self.early_stopping(val_loss, val_acc, epoch, self.histories):\n",
    "                self.print_train_info(epoch, train_acc, train_loss, val_acc, val_loss)\n",
    "            else:\n",
    "                print(\"Early stopping activated\")\n",
    "                print(\"Restoring earlier state and stopping\")\n",
    "                self.early_stopping.print_info()\n",
    "                break\n",
    "\n",
    "    def validate_model(self, loader, model):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in loader:\n",
    "            images_batch = Variable(images, volatile=True)\n",
    "            labels_batch = Variable(labels.long())\n",
    "\n",
    "            if self.cuda_available:\n",
    "                images_batch = images_batch.cuda()\n",
    "                labels_batch = labels_batch.cuda()\n",
    "\n",
    "            output = model(images_batch)\n",
    "            loss = nn.functional.cross_entropy(output, labels_batch.long(), size_average=False)\n",
    "            total_loss += loss.data[0]\n",
    "            total += len(labels_batch)\n",
    "\n",
    "            if not self.cuda_available:\n",
    "                correct += (labels_batch == output.max(1)[1]).data.cpu().numpy().sum()\n",
    "            else:\n",
    "                correct += (labels_batch == output.max(1)[1]).data.sum()\n",
    "        model.train()\n",
    "\n",
    "        average_loss = total_loss / total\n",
    "        return correct / total * 100, average_loss\n",
    "\n",
    "    def print_info(self):\n",
    "        print(self.string_fixer + \" Data \" + self.string_fixer)\n",
    "        print(\"Training set: %d examples\" % (len(self.train_data)))\n",
    "        print(\"Validation set: %d examples\" % (len(self.val_data)))\n",
    "        print(\"Timestamp: %s\" % utils.get_time_hhmmss())\n",
    "\n",
    "        print(self.string_fixer + \" Params \" + self.string_fixer)\n",
    "\n",
    "        print(\"Learning Rate: %f\" % self.params.lr)\n",
    "        print(\"Dropout (p): %f\" % self.params.dropout)\n",
    "        print(\"Batch Size: %d\" % self.params.batch_size)\n",
    "        print(\"Epochs: %d\" % self.params.epochs)\n",
    "        print(\"Patience: %d\" % self.params.patience)\n",
    "        print(\"Resume: %s\" % self.params.resume)\n",
    "\n",
    "    def print_train_info(self, epoch, train_acc, train_loss, val_acc, val_loss):\n",
    "        print((self.string_fixer + \" Epoch: {0}/{1} \" + self.string_fixer)\n",
    "              .format(epoch + 1, self.params.epochs))\n",
    "#         print(\"Train Loss: %.8f, Train Acc: %.2f\" % (train_loss, train_acc))\n",
    "        print(\"Validation Loss: %.8f, Validation Acc: %.2f\" % (val_loss, val_acc))\n",
    "        self.early_stopping.print_info()\n",
    "        print(\"Elapsed Time: %s\" % (utils.get_time_hhmmss(self.start_time)))\n",
    "        print(\"Current timestamp: %s\" % (utils.get_time_hhmmss()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = utils.get_dataset(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train first time with train pickle mentioned in params, should be extended to let the model now about the real distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.resume = False\n",
    "params.extra_debug = False\n",
    "trainer = Trainer(params, train_dataset, val_dataset)\n",
    "trainer.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we append all of the data together and retrain to achieve better validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = utils.load_pickled_data(params.save_loc + 'train_extended_preprocessed.p', [\"features\", \"labels\"])\n",
    "x_load, y_load = utils.load_pickled_data(params.save_loc + 'train_balanced_preprocessed.p', [\"features\", \"labels\"])\n",
    "x = np.append(x, x_load, axis=0)\n",
    "y = np.append(y, y_load, axis=0)\n",
    "\n",
    "x_load, y_load = utils.load_pickled_data(params.save_loc + 'train_balanced_preprocessed_0.5.p', [\"features\", \"labels\"])\n",
    "x = np.append(x, x_load, axis=0)\n",
    "y = np.append(y, y_load, axis=0)\n",
    "\n",
    "x_load, y_load = utils.load_pickled_data(params.save_loc + 'train_balanced_preprocessed_0.25.p', [\"features\", \"labels\"])\n",
    "x = np.append(x, x_load, axis=0)\n",
    "y = np.append(y, y_load, axis=0)\n",
    "\n",
    "x_load, y_load = utils.load_pickled_data(params.save_loc + 'train_balanced_preprocessed_0.10.p', [\"features\", \"labels\"])\n",
    "x = np.append(x, x_load, axis=0)\n",
    "y = np.append(y, y_load, axis=0)\n",
    "\n",
    "x_load, y_load = utils.load_pickled_data(params.save_loc + 'train_balanced_preprocessed_0.05.p', [\"features\", \"labels\"])\n",
    "x = np.append(x, x_load, axis=0)\n",
    "y = np.append(y, y_load, axis=0)\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(x, y, test_size=0.25) \n",
    "train_dataset, val_dataset = TrafficSignsDataset(train_images, train_labels), TrafficSignsDataset(val_images, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params.resume = params.save_loc + \"early_stopping_checkpoint\"\n",
    "params.epochs = 30\n",
    "params.patience = 10\n",
    "params.extra_debug = True\n",
    "trainer = None\n",
    "trainer = Trainer(params, train_dataset, val_dataset)\n",
    "trainer.load()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotter.plot_learning_curves(trainer.histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAEKCAYAAABXMPIIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnCmllFJKOYgmZ0oppZRSDvL/sFaq2a3vW1EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ae3628202e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12631/12631 [04:43<00:00, 44.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate the CSV\n",
    "test_dir = params.data + '/test_images'\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "\n",
    "output_file = open(params.outfile, \"w\")\n",
    "output_file.write(\"Filename,ClassId\\n\")\n",
    "trainer.model.eval()\n",
    "for f in tqdm(os.listdir(test_dir)):\n",
    "    if 'ppm' in f:\n",
    "        data = utils.val_data_transforms(pil_loader(test_dir + '/' + f))\n",
    "        data = data.view(1, data.size(0), data.size(1), data.size(2))\n",
    "        data = data.permute(0, 3, 2, 1)\n",
    "        data, _ = utils.preprocess_dataset(data.numpy(), use_tqdm=False)\n",
    "        data = torch.from_numpy(data).permute(0, 3, 2, 1)\n",
    "        data = Variable(data, volatile=True)\n",
    "        data = data.cuda()\n",
    "        output = trainer.model(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "\n",
    "        file_id = f[0:5]\n",
    "        output_file.write(\"%s,%d\\n\" % (file_id, pred[0][0]))\n",
    "trainer.model.train()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
